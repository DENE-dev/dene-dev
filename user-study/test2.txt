input:great-expectations@great_expectations/great_expectations/dataset/sparkdf_dataset.py
output: line 100 col 

#4
class MetaSparkDFDataset(Dataset):...
class SparkDFDataset(MetaSparkDFDataset):...

#3
logger.debug(str(e))
logger.debug(
    'Unable to load spark context; install optional spark dependency for support.'
    )

#1
import copy
import inspect
import json
import logging
import warnings
from collections import OrderedDict
from datetime import datetime
from functools import reduce, wraps
from typing import List
import jsonschema
import numpy as np
import pandas as pd
from dateutil.parser import parse
from great_expectations.data_asset import DataAsset
from great_expectations.data_asset.util import DocInherit, parse_result_format
from .dataset import Dataset
from .pandas_dataset import PandasDataset
logger = logging.getLogger(__name__)
try:
#6
"""MetaSparkDFDataset is a thin layer between Dataset and SparkDFDataset.
    This two-layer inheritance is required to make @classmethod decorators work.
    Practically speaking, that means that MetaSparkDFDataset implements     expectation decorators, like `column_map_expectation` and `column_aggregate_expectation`,     and SparkDFDataset implements the expectation methods themselves.
    """
def __init__(self, *args, **kwargs):...
@classmethod...
@classmethod...
@classmethod...
#11
"""Constructs an expectation using column-map semantics.


        The MetaSparkDFDataset implementation replaces the "column" parameter supplied by the user with a Spark Dataframe
        with the actual column data. The current approach for functions implementing expectation logic is to append
        a column named "__success" to this dataframe and return to this decorator.

        See :func:`column_map_expectation <great_expectations.Dataset.base.Dataset.column_map_expectation>`         for full documentation of this function.
        """
argspec = inspect.getfullargspec(func)[0][1:]
@cls.expectation(argspec)...
inner_wrapper.__name__ = func.__name__
inner_wrapper.__doc__ = func.__doc__
return inner_wrapper
#13
"""
            This whole decorator is pending a re-write. Currently there is are huge performance issues
            when the # of unexpected elements gets large (10s of millions). Additionally, there is likely
            easy optimization opportunities by coupling result_format with how many different transformations
            are done on the dataset, as is done in sqlalchemy_dataset.
            """
eval_col = '__eval_col_' + column.replace('.', '__').replace('`', '_')
_hidden_res_100_63 = col(column)
self.spark_df = self.spark_df.withColumn(eval_col, _hidden_res_100_63)
if result_format is None:

